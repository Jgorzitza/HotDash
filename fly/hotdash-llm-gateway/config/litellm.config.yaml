# LiteLLM proxy configuration for the HotDash LLM Gateway.
#
# Secrets (OpenAI key, LiteLLM master key, Redis URL, Langfuse keys, etc.)
# are injected at runtime via Fly secrets. All os.environ references below are
# resolved by LiteLLM before the proxy starts.

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  # Separate direct connection for migrations (Supabase port 5432) is provided
  # through DIRECT_URL when Langfuse entrypoints run.
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 300
  store_model_in_db: false

litellm_settings:
  drop_params: true
  json_logs: true
  cache: true
  cache_params:
    type: redis-semantic
    redis_url: os.environ/REDIS_URL
    similarity_threshold: 0.90
    redis_semantic_cache_embedding_model: text-embedding-3-large
    default_in_memory_ttl: 60
    default_in_redis_ttl: 86400
  success_callback:
    - langfuse
  failure_callback:
    - langfuse
  callback_params:
    langfuse:
      host: os.environ/LANGFUSE_HOST
      public_key: os.environ/LANGFUSE_PUBLIC_KEY
      secret_key: os.environ/LANGFUSE_SECRET_KEY
  default_team_settings:
    - team_id: prod_customer
      max_budget: 50
      budget_duration: 1d
      rpm_limit: 100
      tpm_limit: 200000
      metadata: { owner: "ai-customer", queue: "support" }
    - team_id: prod_ceo
      max_budget: 25
      budget_duration: 1d
      rpm_limit: 60
      tpm_limit: 120000
      metadata: { owner: "ceo-insights", queue: "exec" }
    - team_id: prod_background
      max_budget: 10
      budget_duration: 1d
      rpm_limit: 30
      tpm_limit: 60000
      metadata: { owner: "background", queue: "ops" }
    - team_id: admin
      max_budget: 100
      budget_duration: 1d
      rpm_limit: 200
      tpm_limit: 500000
      metadata: { owner: "admin", queue: "override" }

model_list:
  - model_name: gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      timeout: 600
  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      timeout: 600
      priority: 5
  - model_name: text-embedding-3-large
    litellm_params:
      model: text-embedding-3-large
      api_key: os.environ/OPENAI_API_KEY
      timeout: 120

router_settings:
  enable_pre_call_checks: true
  routing_strategy: usage_based_routing
  audit_logs_enabled: true
