# AI-Knowledge 2025-10-15

## Startup & Activation

**13:51 UTC** — Agent startup initiated
**13:55 UTC** — Status changed to ACTIVE with concrete tasks

### Direction Summary
- **Status:** ACTIVE
- **Objective:** Design Knowledge Base Structure and Learning Pipeline
- **Branch:** `agent/ai-knowledge/kb-design`
- **Allowed paths:** `docs/specs/knowledge_base_design.md, feedback/ai-knowledge/*`

### Tasks Assigned
1. ✅ Create feedback file
2. ⏳ Design KB structure in `docs/specs/knowledge_base_design.md`
3. ⏳ Design learning extraction pipeline
4. ⏳ Define quality metrics for KB articles
5. ⏳ Create PR

## Work Log

### 13:55 UTC — Starting KB Design Spec

Creating comprehensive knowledge base design that includes:
- Article format (question, answer, tags, confidence score)
- Categories (orders, shipping, returns, products, technical, policies)
- Update triggers (when to update KB articles)
- Learning extraction from human edits
- Grading data utilization (tone, accuracy, policy)
- Recurring issue identification
- Quality metrics

Next: Create `docs/specs/knowledge_base_design.md`

### 14:00 UTC — KB Design Spec Completed

✅ Created comprehensive `docs/specs/knowledge_base_design.md` (300 lines)

**What was designed:**

1. **KB Article Schema**
   - Core content: question, answer, category, tags
   - Quality metrics: confidence_score, usage_count, success_count
   - Grading: avg_tone_grade, avg_accuracy_grade, avg_policy_grade
   - Search: vector embeddings (OpenAI text-embedding-3-small)
   - 6 categories: orders, shipping, returns, products, technical, policies

2. **Learning Edits Table**
   - Tracks AI draft vs human final version
   - Captures edit distance and ratio
   - Records tone/accuracy/policy grades
   - Links to approvals and KB articles
   - Categorizes learning types: tone_improvement, factual_correction, policy_clarification, template_refinement, new_pattern

3. **Recurring Issues Table**
   - Identifies patterns (≥3 occurrences in 7 days)
   - Tracks resolution status
   - Links to KB articles
   - Measures impact (resolution time, satisfaction)

4. **Learning Extraction Pipeline**
   - Workflow: Draft → Review → Learn
   - Triggers: High-quality approval, significant edits, low grades, recurring patterns
   - Edit analysis with categorization
   - Confidence score formula: weighted average of success rate and grades

5. **Knowledge Retrieval System**
   - Semantic search via embeddings
   - Hybrid search (semantic + keyword)
   - Confidence filtering (≥ 0.60)
   - Context injection for ai-customer agent
   - Usage tracking

6. **Quality Metrics**
   - Per-article: confidence, usage rate, success rate, avg grades
   - System-wide: coverage, draft quality, learning velocity
   - Target: ≥70% coverage, ≥60% minimal edits, ≥5 new articles/week

7. **Update Triggers & Maintenance**
   - Automatic updates on approval
   - Manual review triggers
   - Archival policy (90 days unused + low confidence)

8. **Integration Points**
   - ai-customer agent: KB context in prompts
   - Approvals system: learning extraction on approval
   - Chatwoot: conversation context and linking

9. **Implementation Phases**
   - Phase 1: Foundation (schema, seed, retrieval)
   - Phase 2: Learning Pipeline (edit capture, analysis)
   - Phase 3: Extraction & Creation (KB article creation)
   - Phase 4: Optimization (tuning, monitoring)

10. **Success Criteria & Rollback**
    - Clear DoD for each phase
    - Target metrics aligned with North Star
    - Rollback plan if KB degrades quality

**Key Design Decisions:**

- **Confidence-based filtering:** Only use KB articles with confidence ≥ 0.60
- **Weighted confidence formula:** Success rate (40%) + accuracy grade (30%) + tone grade (20%) + policy grade (10%)
- **Learning triggers:** Automatic on approval, manual review for low confidence
- **Semantic search:** OpenAI embeddings for better matching than keyword-only
- **Hybrid approach:** Combine semantic + keyword search for best results

**Alignment with North Star:**

✅ "Agents propose actions; humans approve or correct; the system learns"
✅ HITL grading (tone/accuracy/policy) drives learning
✅ Evidence-based (edit analysis, confidence scores)
✅ Auditable (learning_edits table, usage tracking)
✅ Rollback plan documented

**Open Questions for Manager:**

1. Confirm OpenAI text-embedding-3-small or alternative?
2. Adjust confidence formula weights?
3. Manual review SLA for flagged articles?
4. Archival threshold (90 days or different)?
5. When to activate KB in ai-customer agent?

Next: Commit and create PR

