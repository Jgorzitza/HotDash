# Reliability Agent Direction
**Updated**: 2025-10-14
**Priority**: GROWTH SPEC EXECUTION - OPERATIONS
**Focus**: Build Monitoring + Alerting Systems

## Mission

Build **monitoring and alerting systems** for growth automation. NOT manually checking systems - build SYSTEMS that monitor automatically.

## Priority 1 - Growth Automation Monitoring

### Task 1: Build KPI Dashboards (4-6 hours)
**Goal**: Real-time visibility into growth metrics

**Requirements** (Growth Spec I1-I8):

**Dashboards to Build**:
1. **Action Throughput Dashboard**
   - Actions created/hour
   - Actions approved/hour
   - Actions executed/hour
   - Backlog size

2. **Recommender Performance Dashboard**
   - Actions generated by type
   - Confidence scores distribution
   - Execution success rate by recommender
   - Processing time metrics

3. **SEO Impact Dashboard**
   - Organic traffic trends
   - CTR improvements
   - Page position changes
   - Search Console metrics

4. **System Health Dashboard**
   - API response times
   - Error rates
   - Queue depths
   - Resource utilization

**Deliverables**:
- [ ] Grafana/monitoring dashboards configured
- [ ] Supabase analytics queries
- [ ] Real-time metric collection
- [ ] Public dashboard URLs
- [ ] GitHub commit (config as code)

### Task 2: Build Alerting System (3-4 hours)
**Goal**: Proactive issue detection

**Alert Rules** (Growth Spec H3):
- Action backlog > 1000 (capacity issue)
- Recommender failure rate > 10% (quality issue)
- Executor error rate > 5% (execution issue)
- Data pipeline stale > 24 hours (freshness issue)
- API error rate > 1% (reliability issue)

**Channels**:
- Slack/email for warnings
- PagerDuty for critical
- Dashboard annotations

**Deliverables**:
- [ ] Alert rules configured
- [ ] Notification channels set up
- [ ] Escalation policies defined
- [ ] Test alerts verified
- [ ] Runbooks linked to alerts

### Task 3: Build SLO Tracking (3-4 hours)
**Goal**: Service Level Objective monitoring

**SLOs to Track** (Growth Spec I):
- Action API availability: 99.9%
- Action approval latency: p95 < 500ms
- Recommender success rate: > 95%
- Executor success rate: > 98%
- Data freshness: < 6 hours

**Deliverables**:
- [ ] SLO definitions in code
- [ ] Error budget calculations
- [ ] SLO dashboards
- [ ] Burn rate alerts
- [ ] GitHub commit

## Priority 2 - Incident Response Automation

### Task 4: Build Auto-Remediation (4-6 hours)
**Goal**: Automated incident response

**Auto-Remediation Rules**:
```typescript
// app/services/auto-remediation.server.ts
const remediationRules = [
  {
    condition: 'queue_depth > 10000',
    action: 'scale_workers_up',
    params: { targetWorkers: 10 }
  },
  {
    condition: 'error_rate > 10%',
    action: 'circuit_breaker_open',
    params: { service: 'recommenders' }
  },
  {
    condition: 'data_stale > 24h',
    action: 'trigger_pipeline_refresh',
    params: { pipeline: 'gsc' }
  }
];
```

**Deliverables**:
- [ ] Auto-remediation service
- [ ] Safe remediation actions (no destructive)
- [ ] Audit log of auto-remediations
- [ ] Manual override capability
- [ ] GitHub commit

### Task 5: Build Chaos Engineering Tests (4-6 hours)
**Goal**: Validate resilience

**Chaos Scenarios**:
- Kill recommender process (should restart)
- Simulate Shopify API timeout (should retry)
- Inject bad data (should validate and reject)
- Overload Action queue (should throttle)

**Deliverables**:
- [ ] Chaos test suite
- [ ] Failure injection tools
- [ ] Recovery validation
- [ ] Resilience report
- [ ] GitHub commit

## Priority 3 - Performance Optimization

### Task 6: Build Performance Profiling (3-4 hours)
**Goal**: Identify and fix bottlenecks

**Profiling Targets**:
- Recommender processing time
- Action execution duration
- API response times
- Database query performance

**Deliverables**:
- [ ] APM integration (Datadog/New Relic or built-in)
- [ ] Slow query identification
- [ ] Performance baselines
- [ ] Optimization recommendations
- [ ] GitHub commit

## Build Monitoring Automation, Not Manual Checks

**✅ RIGHT**:
- Build dashboards (automated visibility)
- Build alerting (automated detection)
- Build auto-remediation (automated response)

**❌ WRONG**:
- Manually check logs
- Manually monitor metrics
- Manual incident response

## Evidence Required

- Dashboard screenshots/URLs
- Alert rule configurations
- SLO tracking proof
- Auto-remediation logs
- Chaos test results

## Success Criteria

**Week 1 Complete When**:
- [ ] KPI dashboards operational (I1-I8)
- [ ] Alerting system live
- [ ] SLOs tracked and visualized
- [ ] Auto-remediation for common issues
- [ ] Chaos tests validating resilience

## Report Every 2 Hours

Update `feedback/reliability.md`:
- Monitoring systems built
- Alerts configured
- Incidents detected/resolved
- Performance optimizations
- Evidence (dashboards, alerts, commits)

---

**Remember**: Build MONITORING AUTOMATION that detects and responds to issues automatically, not manual runbooks. Build systems that scale.
